{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = [\"n/a\", \"na\", \"--\",\".\",\"NaN\"]\n",
    "df_t_list = []\n",
    "df_p_list = []\n",
    "for index in range (3,7):\n",
    "    df_in = pd.read_csv(\"Data/training201{}.csv\".format(index), na_values = missing_values)  \n",
    "    df_in2 = pd.read_csv(\"Data/prediction201{}.csv\".format(index), na_values = missing_values)\n",
    "    \n",
    "    df_in2 = df_in2.drop([\"CHGENROL\",\"FRACDEGRE\",\"RETPCNT\"],axis = 1)\n",
    "    ##drop prediction rows from predictions\n",
    "    df_t_list.append(df_in)\n",
    "    df_p_list.append(df_in2)\n",
    "\n",
    "#read the example submission in\n",
    "example_submission = pd.read_csv(\"Data/ex_submit.csv\")\n",
    "submission_col = example_submission.columns \n",
    "submission_shape = example_submission.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inpute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, df in enumerate(df_t_list):\n",
    "    df_t_list[index] = df.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "for index, df in enumerate(df_p_list):\n",
    "    df_p_list[index] = df.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperate the data into X and y and X_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"CHGENROL\",\"FRACDEGRE\",\"RETPCNT\"]\n",
    "X_list = []\n",
    "X_p_list = []\n",
    "y_list = []\n",
    "#drop the seperate the data into targets and data\n",
    "\n",
    "\n",
    "for index, (df_t, df_p) in enumerate(zip(df_t_list,df_p_list)):\n",
    "    cols = [col for col in df_t_list[index].columns if col not in target_cols]\n",
    "    X_list.append(df_t[cols])\n",
    "    y_list.append(df_t[target_cols])\n",
    "    X_p_list.append(df_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nrobust_scaler = preprocessing.RobustScaler()\\nnp_scaled = robust_scaler.fit_transform(dataset[collist[1:]])\\n\\nnp_scaled = dataset[collist[1:]].values\\nfill_Nan = SimpleImputer(missing_values = np.nan, strategy =\"constant\", fill_value = -50.0)\\nnp_imp = fill_Nan.fit_transform(np_scaled)\\ndataset_scaled = pd.DataFrame(np_imp)\\n\\n# create the Labelencoder object\\nle = preprocessing.LabelEncoder()\\ndataset[\"File\"] = le.fit_transform(dataset[\"File\"])\\ndataset[\"File\"].unique()\\n\\n#merge the two dataframes\\ndataset_scaled[\"33\"] = dataset[\"File\"]\\n\\n##Handle the prediction data\\n\\n##robust_scaler2 = preprocessing.RobustScaler()\\n##np_scaled2 = robust_scaler.fit_transform(dataset2[collist[1:]])\\nnp_scaled2 = dataset2[collist[1:]].values\\n\\nfill_Nan2 = SimpleImputer(missing_values = np.nan, strategy =\"constant\", fill_value = -50.0)\\nnp_imp2 = fill_Nan2.fit_transform(np_scaled2)\\ndataset_scaled2 = pd.DataFrame(np_imp2)\\n\\n# create the Labelencoder object\\nle2 = preprocessing.LabelEncoder()\\ndataset2[\"File\"] = le2.fit_transform(dataset2[\"File\"])\\ndataset2[\"File\"].unique()\\n\\n#merge the two dataframes\\ndataset_scaled2[\"33\"] = dataset2[\"File\"]\\ndataset_scaled2.head(4)\\n\\n'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for index, X in enumerate(X_list):\n",
    "    minmax = MinMaxScaler()\n",
    "    X_list[index] = minmax.fit_transform(X)\n",
    "    \n",
    "for index, X in enumerate(X_p_list):\n",
    "    minmax = MinMaxScaler()\n",
    "    X_p_list[index] = minmax.fit_transform(X)\n",
    "\n",
    "\"\"\"\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "np_scaled = robust_scaler.fit_transform(dataset[collist[1:]])\n",
    "\n",
    "np_scaled = dataset[collist[1:]].values\n",
    "fill_Nan = SimpleImputer(missing_values = np.nan, strategy =\"constant\", fill_value = -50.0)\n",
    "np_imp = fill_Nan.fit_transform(np_scaled)\n",
    "dataset_scaled = pd.DataFrame(np_imp)\n",
    "\n",
    "# create the Labelencoder object\n",
    "le = preprocessing.LabelEncoder()\n",
    "dataset[\"File\"] = le.fit_transform(dataset[\"File\"])\n",
    "dataset[\"File\"].unique()\n",
    "\n",
    "#merge the two dataframes\n",
    "dataset_scaled[\"33\"] = dataset[\"File\"]\n",
    "\n",
    "##Handle the prediction data\n",
    "\n",
    "##robust_scaler2 = preprocessing.RobustScaler()\n",
    "##np_scaled2 = robust_scaler.fit_transform(dataset2[collist[1:]])\n",
    "np_scaled2 = dataset2[collist[1:]].values\n",
    "\n",
    "fill_Nan2 = SimpleImputer(missing_values = np.nan, strategy =\"constant\", fill_value = -50.0)\n",
    "np_imp2 = fill_Nan2.fit_transform(np_scaled2)\n",
    "dataset_scaled2 = pd.DataFrame(np_imp2)\n",
    "\n",
    "# create the Labelencoder object\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "dataset2[\"File\"] = le2.fit_transform(dataset2[\"File\"])\n",
    "dataset2[\"File\"].unique()\n",
    "\n",
    "#merge the two dataframes\n",
    "dataset_scaled2[\"33\"] = dataset2[\"File\"]\n",
    "dataset_scaled2.head(4)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_model(X, y):\n",
    "    MLP = MLPRegressor(learning_rate='adaptive', max_iter=1000, random_state=4, warm_start = True)\n",
    "    params = {\n",
    "    \"solver\":[\"lbfgs\",\"adam\"],\n",
    "    'activation': ['logistic', 'relu', 'tanh'],  # <-- added 'tanh' as third non-linear activation function\n",
    "    'alpha': np.logspace(0.0001, 100, 10),\n",
    "    'hidden_layer_sizes': [\n",
    "        (10, 10), (20, 10), (30, 10),\n",
    "        (40, 10), (90, 10), (90, 30, 10),(100,10,10,10),(30,20,10),(60,30,30,30,3)  # <-- added more neurons or layers\n",
    "    ]\n",
    "    }\n",
    "    clf = GridSearchCV(estimator=MLP, param_grid=params,\n",
    "                   n_jobs=-1, cv=3, refit=True, verbose=5) \n",
    "    \n",
    "    clf.fit(X, y)\n",
    "   \n",
    "    return clf, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 36 concurrent workers.\n",
      "exception calling callback for <Future at 0x211926896d8 state=finished raised TerminatedWorkerError>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\_base.py\", line 625, in _invoke_callbacks\n",
      "    callback(self)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 309, in __call__\n",
      "    self.parallel.dispatch_next()\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 731, in dispatch_next\n",
      "    if not self.dispatch_one_batch(self._original_iterator):\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 759, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 716, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 510, in apply_async\n",
      "    future = self._workers.submit(SafeFunction(func))\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\reusable_executor.py\", line 151, in submit\n",
      "    fn, *args, **kwargs)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 1022, in submit\n",
      "    raise self._flags.broken\n",
      "sklearn.externals.joblib.externals.loky.process_executor.TerminatedWorkerError: A worker process managed by the executor was unexpectedly terminated. This could be caused by a segmentation fault while calling the function or by an excessive memory usage causing the Operating System to kill the worker.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "clf_list = []\n",
    "for X , y in zip(X_list,y_list):\n",
    "    clf, best_params = MLP_model(X,y)\n",
    "    clf_list.append((clf,best_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\utils.py:55: UserWarning: Failed to kill subprocesses on this platform. Pleaseinstall psutil: https://github.com/giampaolo/psutil\n",
      "  warnings.warn(\"Failed to kill subprocesses on this platform. Please\"\n",
      "Exception in thread QueueManagerThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 674, in _queue_management_worker\n",
      "    recursive_terminate(p)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\utils.py\", line 28, in recursive_terminate\n",
      "    _recursive_terminate_without_psutil(process)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\utils.py\", line 53, in _recursive_terminate_without_psutil\n",
      "    _recursive_terminate(process.pid)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\backend\\utils.py\", line 73, in _recursive_terminate\n",
      "    stderr=None)\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\subprocess.py\", line 395, in check_output\n",
      "    **kwargs).stdout\n",
      "  File \"C:\\Users\\Robert\\.conda\\envs\\sklearn\\lib\\subprocess.py\", line 487, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['taskkill', '/F', '/T', '/PID', '15056']' returned non-zero exit status 3221225794.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done  90 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=36)]: Done 293 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=36)]: Done 576 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=36)]: Done 941 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=36)]: Done 1386 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=36)]: Done 1913 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=36)]: Done 2000 out of 2000 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "predict_raw2 = rfr2.predict(X_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1188.000000</td>\n",
       "      <td>1188.000000</td>\n",
       "      <td>1188.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.001158</td>\n",
       "      <td>0.203068</td>\n",
       "      <td>74.493330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.016778</td>\n",
       "      <td>0.031128</td>\n",
       "      <td>9.463537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.914500</td>\n",
       "      <td>0.091587</td>\n",
       "      <td>43.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.994055</td>\n",
       "      <td>0.185336</td>\n",
       "      <td>67.193375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000659</td>\n",
       "      <td>0.199732</td>\n",
       "      <td>74.045750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.007852</td>\n",
       "      <td>0.218781</td>\n",
       "      <td>80.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.166484</td>\n",
       "      <td>0.346628</td>\n",
       "      <td>97.386500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2\n",
       "count  1188.000000  1188.000000  1188.000000\n",
       "mean      1.001158     0.203068    74.493330\n",
       "std       0.016778     0.031128     9.463537\n",
       "min       0.914500     0.091587    43.991000\n",
       "25%       0.994055     0.185336    67.193375\n",
       "50%       1.000659     0.199732    74.045750\n",
       "75%       1.007852     0.218781    80.908500\n",
       "max       1.166484     0.346628    97.386500"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_p1 = pd.DataFrame(predict_raw)\n",
    "y_p2 = pd.DataFrame(predict_raw2)\n",
    "\n",
    "y_p2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_labes = dataset[\"File\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_2013_np = predict_raw2[file_labels[file_labels == 0]]\n",
    "p_2014_np = predict_raw2[file_labels[file_labels == 1]]\n",
    "p_2015_np = predict_raw2[file_labels[file_labels == 2]]\n",
    "p_2016_np = predict_raw2[file_labels[file_labels == 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuid = dataset2[\"NUID\"].values\n",
    "nuid = nuid[:297]\n",
    "nuid= nuid.reshape(297,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9577.0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_all = np.hstack((nuid,p_2013_np, p_2014_np, p_2015_np, p_2016_np))\n",
    "p_df = pd.DataFrame(p_all)\n",
    "#test if same nuid\n",
    "#p_df[0][k%297]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sub = pd.read_csv(\"Data/ex_submit.csv\")\n",
    "p_df.columns = example_sub.columns \n",
    "#test of same nuid\n",
    "#dataset2[\"NUID\"][k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv = p_df.to_csv (r'predictions1.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np_scaled2[:,26:29] = predict_raw\n",
    "np_unscaled2 = robust_scaler.inverse_transform(np_scaled2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_p_unsc = np_unscaled2[:,26:29]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_p_unsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_p = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_t = y.values\n",
    "#y_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def RMSE(targets, predictions):\n",
    " #   return np.sqrt(np.mean((predictions-targets)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE(y_t,y_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\"\"\"np_scaled_2 = np_scaled.copy()\n",
    "np_unscaled = robust_scaler.inverse_transform(np_scaled)\n",
    "\n",
    "np_scaled_2[:,26:29] = y_p\n",
    "np_unscaled_p = robust_scaler.inverse_transform(np_scaled_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_t_unsc = np_unscaled[:,26:29]\n",
    "#y_p_unsc = np_unscaled_p[:,26:29]\n",
    "#y_p_unsc[0:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_t_unsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_p_unsc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE(y_t_unsc, y_p_unsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
